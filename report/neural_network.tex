\Paragraph{Neural Network}
Since the feature contains the sentence structure, the model is designed to process having capability of recognize which word come first and later instead up allowing model randomly search the best combination. Causal convolution layer \ref{causal}, a type of convolution, has been used in this problem to maintain this time series like structure. In order to having a larger size of reception field, dilation blocks are used, in which each layer extract certain amount of feature with sequence information \ref{dilation}, so that it is quite possible to process a much longer sentence without changing the structure with additional information. Residual network structure has been used on dilation block to prevent gradient vanishing in deep network.
The overall model structure is a forward causal layer to process sentence in a forward sequence, followed by two dilation residual blocks. There is a parallel structure to process the sentence backward. Two dilation blocks skip connections on both sequence are concatenated, then into a fully connected layer and enters a Softmax layer to make classification prediction.
Because the limitation of dataset and purpose of in comparison with other modeling method, the model parameters used in the network for comparison are two dilation blocks with each only two layers to process four words. Only forward processing blocks exist, since the dataset is too small, which has been under-fitting. Number of filters of both causal convolution and dilation layers is 8. Number of filters of dense layer before the softmax layer is 16. The data is trained the whole dataset of a word per batch, and repeated five times for the same dataset.